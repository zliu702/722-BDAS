{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binomial Logistic Regression (Documentation Example)\n",
    "\n",
    "The documentation example is available here: https://spark.apache.org/docs/2.1.1/ml-classification-regression.html#logistic-regression.\n",
    "\n",
    "Binomial logistic regression is a classification algorithm, unlike linear regression. It can be used for binary classification (predicting a 1 or a 0). \n",
    "\n",
    "Objective: Let's see a simple example of how to run binomial logistic regression with Python and Spark. While this documentation dataset is structured/unrealistic, it provides a basic summary of how to utilise logistic regression. For a more realistic exercise, move on to the advanced logistic regression exercise. Please note that there is a huge amount of documentation available online. Ensure that you understand what you're doing and why. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section must be included at the beginning of each new notebook. Remember to change the app name.\n",
    "# If you're using VirtualBox, change the below to '/home/user/spark-2.1.1-bin-hadoop2.7'\n",
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-2.1.1-bin-hadoop2.7')\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('logistic_regression_docs').getOrCreate()\n",
    "\n",
    "# If you're getting an error with numpy, please type 'sudo pip3 install numpy --user' into the console.\n",
    "# If you're getting an error with another package, type 'sudo pip3 install PACKAGENAME --user'. \n",
    "# Replace PACKAGENAME with the relevant package (such as pandas, etc).\n",
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n",
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = true)\n",
      "\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|(692,[127,128,129...|[19.8534775947478...|[0.99999999761359...|       0.0|\n",
      "|  1.0|(692,[158,159,160...|[-20.377398194908...|[1.41321555111056...|       1.0|\n",
      "|  1.0|(692,[124,125,126...|[-27.401459284891...|[1.25804865126979...|       1.0|\n",
      "|  1.0|(692,[152,153,154...|[-18.862741612668...|[6.42710509170303...|       1.0|\n",
      "|  1.0|(692,[151,152,153...|[-20.483011833009...|[1.27157209200604...|       1.0|\n",
      "|  0.0|(692,[129,130,131...|[19.8506078990277...|[0.99999999760673...|       0.0|\n",
      "|  1.0|(692,[158,159,160...|[-20.337256674833...|[1.47109814695581...|       1.0|\n",
      "|  1.0|(692,[99,100,101,...|[-19.595579753418...|[3.08850168102631...|       1.0|\n",
      "|  0.0|(692,[154,155,156...|[19.2708803215613...|[0.99999999572670...|       0.0|\n",
      "|  0.0|(692,[127,128,129...|[23.6202328360422...|[0.99999999994480...|       0.0|\n",
      "|  1.0|(692,[154,155,156...|[-24.385235147661...|[2.56818872776510...|       1.0|\n",
      "|  0.0|(692,[153,154,155...|[26.3082522490179...|[0.99999999999624...|       0.0|\n",
      "|  0.0|(692,[151,152,153...|[25.8329060318703...|[0.99999999999396...|       0.0|\n",
      "|  1.0|(692,[129,130,131...|[-19.794609139086...|[2.53110684529575...|       1.0|\n",
      "|  0.0|(692,[154,155,156...|[21.0260440948067...|[0.99999999926123...|       0.0|\n",
      "|  1.0|(692,[150,151,152...|[-22.764979942873...|[1.29806018790941...|       1.0|\n",
      "|  0.0|(692,[124,125,126...|[21.5049307193954...|[0.99999999954235...|       0.0|\n",
      "|  0.0|(692,[152,153,154...|[31.9927184226421...|[0.99999999999998...|       0.0|\n",
      "|  1.0|(692,[97,98,99,12...|[-20.521067180414...|[1.22409115616505...|       1.0|\n",
      "|  1.0|(692,[124,125,126...|[-22.245377742755...|[2.18250475400332...|       1.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load training data. Libsvm is used throughout the Spark documentation, but it's probably not relevant to your dataset. \n",
    "training = spark.read.format(\"libsvm\").load(\"Datasets/sample_libsvm_data.txt\")\n",
    "\n",
    "# Print schema to get a high level view of the schema. Simply label and features. \n",
    "training.printSchema()\n",
    "\n",
    "# Create an instance of the logistic regression model. This is where you specify features/label/prediction columns.\n",
    "# The documentation example includes three parameters. We'll leave those out for now, but they're important to specify in your assignment.\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# Fit the model. Note that the documentation example doesn't split the data into training/testing/validation sets.\n",
    "lrModel = lr.fit(training)\n",
    "\n",
    "# Instead of printing the coefficients/intercept, let's try to get a high level view of the model output. \n",
    "trainingSummary = lrModel.summary\n",
    "\n",
    "# Print schema again. Note the additional columns. As with other models, we simply want to compare the label (actual value) to the predicted value.\n",
    "trainingSummary.predictions.printSchema()\n",
    "\n",
    "# Visualise the DataFrame. Visually compare label to prediction. What do you see? \n",
    "trainingSummary.predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Train/Test Split\n",
    "\n",
    "While this isn't part of the documentation example, it's important to understand how to do a train/test split in order to evaluate the model's results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|(692,[95,96,97,12...|[22.7469390876039...|[0.99999999986783...|       0.0|\n",
      "|  0.0|(692,[121,122,123...|[19.5758692957412...|[0.99999999685001...|       0.0|\n",
      "|  0.0|(692,[122,123,124...|[16.9339400170149...|[0.99999995577342...|       0.0|\n",
      "|  0.0|(692,[123,124,125...|[34.1630647521288...|[0.99999999999999...|       0.0|\n",
      "|  0.0|(692,[123,124,125...|[30.1535882634884...|[0.99999999999991...|       0.0|\n",
      "|  0.0|(692,[124,125,126...|[28.6898147896084...|[0.99999999999965...|       0.0|\n",
      "|  0.0|(692,[124,125,126...|[24.3959410526453...|[0.99999999997459...|       0.0|\n",
      "|  0.0|(692,[124,125,126...|[21.6637401587347...|[0.99999999960955...|       0.0|\n",
      "|  0.0|(692,[125,126,127...|[25.7020255947206...|[0.99999999999311...|       0.0|\n",
      "|  0.0|(692,[126,127,128...|[29.9000451493579...|[0.99999999999989...|       0.0|\n",
      "|  0.0|(692,[126,127,128...|[33.9785071617155...|[0.99999999999999...|       0.0|\n",
      "|  0.0|(692,[127,128,129...|[23.8736496132152...|[0.99999999995716...|       0.0|\n",
      "|  0.0|(692,[127,128,129...|[22.8778445918405...|[0.99999999988404...|       0.0|\n",
      "|  0.0|(692,[152,153,154...|[28.7628972189163...|[0.99999999999967...|       0.0|\n",
      "|  0.0|(692,[152,153,154...|[14.3232426767306...|[0.99999939814138...|       0.0|\n",
      "|  0.0|(692,[153,154,155...|[30.7401903761174...|[0.99999999999995...|       0.0|\n",
      "|  0.0|(692,[153,154,155...|[30.0578008477610...|[0.99999999999991...|       0.0|\n",
      "|  0.0|(692,[181,182,183...|[17.7477069904285...|[0.99999998039942...|       0.0|\n",
      "|  0.0|(692,[234,235,237...|[5.81298733930486...|[0.99702041536301...|       0.0|\n",
      "|  1.0|(692,[97,98,99,12...|[-18.036625890486...|[1.46822595520018...|       1.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's split the data into a training/testing split so we can evalaute the model.\n",
    "lr_train,lr_test = training.randomSplit([0.7,0.3])\n",
    "\n",
    "# Create instance of the logistic regression model.\n",
    "final_model = LogisticRegression()\n",
    "\n",
    "# Now fit the model on a subset of data.\n",
    "fit_final = final_model.fit(lr_train)\n",
    "\n",
    "# And evaluate it against the test data.\n",
    "predictions_and_labels = fit_final.evaluate(lr_test)\n",
    "\n",
    "predictions_and_labels.predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Evaluation Metrics\n",
    "\n",
    "Evaluators are an important when working with machine learning, let's see some basics for Logistic Regression. Check out these links:\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.evaluation.BinaryClassificationEvaluator\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "\n",
    "For a binary evaluator, you can get the area under the ROC curve or the area under the precision/recall curve.\n",
    "\n",
    "For a multi-class evalautor, you can get back accuracy, precision/recall, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import one evalulation metrics. \n",
    "# Remember, binary is for true and false (0 and 1) predictions, while multi-class is for multiple classification classes.\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an instance of the evaluator (finds area under the curve).\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "\n",
    "# Evaluate the predictions. \n",
    "my_final_roc = evaluator.evaluate(predictions_and_labels.predictions)\n",
    "\n",
    "# Display the results. \n",
    "my_final_roc\n",
    "\n",
    "# According to this evaluation metric, the area under the curve is 1.0. A perfect fit? Is that realistic?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the unrealistic evaluation metric results are because of the test dataset. Move on to the Advanced Logistic Regression lab to start using a more realistic dataset. \n",
    "\n",
    "Applying Decision Trees is similar to applying Logistic Regression. Find out more here: https://spark.apache.org/docs/2.1.1/ml-classification-regression.html#decision-tree-classifier"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
