{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression (Documentation Example)\n",
    "\n",
    "The documentation example is available here: https://spark.apache.org/docs/latest/ml-classification-regression.html#linear-regression\n",
    "\n",
    "Logistic regression is a classification algorithm, unlike linear regression. \n",
    "\n",
    "Objective: Let's see an example of how to run a logistic regression with Python and Spark. While this documentation dataset is unrealistic, it provides a basic summary of how to use logistic regression. For a more realistic exercise, move on to the advanced logistic regression exercise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Must be included at the beginning of each new notebook. Remember to change the app name.\n",
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-3.2.1-bin-hadoop2.7')\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('logistic_regression_docs').getOrCreate()\n",
    "\n",
    "# If you're getting an error with numpy, please type 'sudo pip install numpy --user' into the EC2 console.\n",
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load training data. Libsvm is used throughout the Spark documentation.\n",
    "# Libsvm is probably not relevant to your dataset. \n",
    "training = spark.read.format(\"libsvm\").load(\"Datasets/sample_libsvm_data.txt\")\n",
    "\n",
    "# Instance of logistic regression model. This is where you specify features/label/prediction columns.\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# Fit the model. Note that the train/test split isn't part of the documentation example.\n",
    "lrModel = lr.fit(training)\n",
    "\n",
    "trainingSummary = lrModel.summary\n",
    "\n",
    "# Raw prediction and probability have to do with logistic regression. \n",
    "# As with other models, we simply want to compare the label (actual) to the prediction.\n",
    "trainingSummary.predictions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|(692,[127,128,129...|[19.8534775947478...|[0.99999999761359...|       0.0|\n",
      "|  1.0|(692,[158,159,160...|[-20.377398194908...|[1.41321555111056...|       1.0|\n",
      "|  1.0|(692,[124,125,126...|[-27.401459284891...|[1.25804865126979...|       1.0|\n",
      "|  1.0|(692,[152,153,154...|[-18.862741612668...|[6.42710509170303...|       1.0|\n",
      "|  1.0|(692,[151,152,153...|[-20.483011833009...|[1.27157209200604...|       1.0|\n",
      "|  0.0|(692,[129,130,131...|[19.8506078990277...|[0.99999999760673...|       0.0|\n",
      "|  1.0|(692,[158,159,160...|[-20.337256674833...|[1.47109814695581...|       1.0|\n",
      "|  1.0|(692,[99,100,101,...|[-19.595579753418...|[3.08850168102631...|       1.0|\n",
      "|  0.0|(692,[154,155,156...|[19.2708803215613...|[0.99999999572670...|       0.0|\n",
      "|  0.0|(692,[127,128,129...|[23.6202328360422...|[0.99999999994480...|       0.0|\n",
      "|  1.0|(692,[154,155,156...|[-24.385235147661...|[2.56818872776510...|       1.0|\n",
      "|  0.0|(692,[153,154,155...|[26.3082522490179...|[0.99999999999624...|       0.0|\n",
      "|  0.0|(692,[151,152,153...|[25.8329060318703...|[0.99999999999396...|       0.0|\n",
      "|  1.0|(692,[129,130,131...|[-19.794609139086...|[2.53110684529575...|       1.0|\n",
      "|  0.0|(692,[154,155,156...|[21.0260440948067...|[0.99999999926123...|       0.0|\n",
      "|  1.0|(692,[150,151,152...|[-22.764979942873...|[1.29806018790941...|       1.0|\n",
      "|  0.0|(692,[124,125,126...|[21.5049307193954...|[0.99999999954235...|       0.0|\n",
      "|  0.0|(692,[152,153,154...|[31.9927184226421...|[0.99999999999998...|       0.0|\n",
      "|  1.0|(692,[97,98,99,12...|[-20.521067180414...|[1.22409115616505...|       1.0|\n",
      "|  1.0|(692,[124,125,126...|[-22.245377742755...|[2.18250475400332...|       1.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Label and prediction are stacked on each other. \n",
    "trainingSummary.predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|(692,[100,101,102...|[8.26774799904665...|[0.99974340321337...|       0.0|\n",
      "|  0.0|(692,[122,123,124...|[19.5940219685167...|[0.99999999690668...|       0.0|\n",
      "|  0.0|(692,[124,125,126...|[28.9430106369702...|[0.99999999999973...|       0.0|\n",
      "|  0.0|(692,[124,125,126...|[21.1603318310783...|[0.99999999935407...|       0.0|\n",
      "|  0.0|(692,[125,126,127...|[18.7156105338896...|[0.99999999255416...|       0.0|\n",
      "|  0.0|(692,[126,127,128...|[17.7370325045155...|[0.99999998018907...|       0.0|\n",
      "|  0.0|(692,[126,127,128...|[25.3330396245836...|[0.99999999999004...|       0.0|\n",
      "|  0.0|(692,[129,130,131...|[15.9433351174379...|[0.99999988090391...|       0.0|\n",
      "|  0.0|(692,[153,154,155...|[26.9841698807831...|[0.99999999999809...|       0.0|\n",
      "|  0.0|(692,[153,154,155...|[13.2100268962704...|[0.99999816786522...|       0.0|\n",
      "|  0.0|(692,[154,155,156...|[8.74291755196778...|[0.99984043786122...|       0.0|\n",
      "|  0.0|(692,[181,182,183...|[14.8888329437470...|[0.99999965812931...|       0.0|\n",
      "|  1.0|(692,[99,100,101,...|[-10.419950512923...|[2.98304658170971...|       1.0|\n",
      "|  1.0|(692,[123,124,125...|[-25.654036124208...|[7.22093894124607...|       1.0|\n",
      "|  1.0|(692,[123,124,125...|[-21.376848195544...|[5.20178967100249...|       1.0|\n",
      "|  1.0|(692,[124,125,126...|[-26.856279847688...|[2.17003059870240...|       1.0|\n",
      "|  1.0|(692,[124,125,126...|[-18.185981779879...|[1.26452802706719...|       1.0|\n",
      "|  1.0|(692,[124,125,126...|[-20.945799363411...|[8.00488168120690...|       1.0|\n",
      "|  1.0|(692,[126,127,128...|[-29.301269070130...|[1.88200394774070...|       1.0|\n",
      "|  1.0|(692,[126,127,128...|[-20.793039591870...|[9.32604792411605...|       1.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's split the data so we can evalaute the model.\n",
    "lr_train,lr_test = training.randomSplit([0.7,0.3])\n",
    "\n",
    "final_model = LogisticRegression()\n",
    "\n",
    "# Now we're fitting the model on a subset of data.\n",
    "fit_final = final_model.fit(lr_train)\n",
    "\n",
    "# And evaluating it against the test data.\n",
    "predictions_and_labels = fit_final.evaluate(lr_test)\n",
    "\n",
    "predictions_and_labels.predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Evaluation Metrics\n",
    "\n",
    "Evaluators are an important part of our pipline when working with Machine Learning, let's see some basics for Logistic Regression. Check out these links:\n",
    "\n",
    "For a binary evaluator, you can get the area under the ROC curve or the area under the precision/recall curve.\n",
    "\n",
    "For a multi-class evalautor, you can get back accuracy, precision/recall, etc. \n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.evaluation.BinaryClassificationEvaluator\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.evaluation.MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import two evalulation metrics. \n",
    "# Remember, binary is for predictions like true and false (0 and 1), \n",
    "# While multi-class is for multiple classification classes.\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator,MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# According to this evaluation metric, the area under the curve is 1.0. A perfect fit? Is that realistic?  \n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "my_final_roc = evaluator.evaluate(predictions_and_labels.predictions)\n",
    "my_final_roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For multiclass.\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='label',\n",
    "                                             metricName='accuracy')\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions_and_labels.predictions)\n",
    "\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the high evaluation metrics are because of the test dataset. Move on to the next logistic regression lab to start using a more realistic dataset. \n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
